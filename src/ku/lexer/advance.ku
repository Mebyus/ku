import std {
	chars -> "chars"
}

// Shift lexer scan position one byte forward.
fun (&Lexer) advance() {
	g.pos += 1;
}

// Start recording new token data.
fun (&Lexer) start() {
	g.mark = g.pos;
}

// Returns a view (slice into scanned text) of recorded token data.
fun (&Lexer) view() -> []u8 {
	ret g.text[g.mark:g.pos];
}

// Returns recorded token data string (lit, true).
//
// Returns ("", false) in case token length overflowed max length.
fun (&Lexer) take() -> (str, bool) {
	if g.is_length_overflow() {
		ret "", false;
	}
	ret cast(str, g.view()), true;
}

// Returns byte length of recorded token data.
fun (&Lexer) len() -> u32 {
	ret lx.pos - lx.mark;
}

const max_token_byte_length := 1 << 12;

fun (&Lexer) is_length_overflow() -> bool {
	ret g.len() > max_token_byte_length;
}

fun (&Lexer) skip_word() {
	for !g.eof() && chars.is_alphanum(g.peek()) {
		g.advance();
	}
}

fun (&Lexer) skip_hex_digits() {
	for !g.eof() && chars.is_hex_digit(g.peek()) {
		g.advance();
	}
}

fun (&Lexer) skip_line_comment() {
	g.advance(); // skip '/'
	g.advance(); // skip '/'
	g.skip_line();
}

fun (&Lexer) skip_block_comment() {
	g.advance(); // skip '/'
	g.advance(); // skip '*'

	for !g.eof() && !(g.peek() == '*' && g.next() == '/') {
		g.advance();
	}

	if g.eof() {
		ret;
	}

	g.advance(); // skip '*'
	g.advance(); // skip '/'
}

fun (&Lexer) skip_whitespace_and_comments() {
	for {
		g.skip_whitespace();
		if g.eof() {
			ret;
		}

		if g.peek() == '/' && g.next() == '/' {
			g.skip_line_comment();
		} else if g.peek() == '/' && g.next() == '*' {
			g.skip_block_comment();
		} else {
			ret;
		}
	}
}

fun (&Lexer) skip_whitespace() {
	for !g.eof() && chars.is_whitespace(g.peek()) {
		g.advance();
	}
}

fun (&Lexer) skip_line() {
	for !g.eof() && g.peek() != '\n' {
		g.advance();
	}
	if g.eof() {
		ret;
	}

	g.advance(); // skip newline byte
}
